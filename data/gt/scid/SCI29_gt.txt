Abstract
small black and brown dog play with red ball in the grass
dog play with a red ball in the grass
In this paper, we propose multimodal convolutional neu- ral networks (m-CNNs) for matching image and sentence. Our m-CNN provides an end-to-end framework with convo- lutional architectures to exploit image representation, word composition, and the matching relations between the two modalities. More specifically, it consists of one image CNN encoding the image content and one matching CNN mod- eling the joint representation of image and sentence. The matching CNN composes different semantic fragments from words and learns the inter-modal relations between image and the composed fragments at different levels, thus ful- ly exploit the matching relations between image and sen- tence. Experimental results demonstrate that the proposed m-CNNs can effectively capture the information necessary for image and sentence matching. More specifically, our proposed m-CNNs significantly outperform the state-of- the-art approaches for bidirectional image and sentence re- trieval on the Flickr8K and Flickr3OK datasets.
small black and brown dog play with a red ball
a red ball
black and brown dog
grass
ball
dog
Figure 1. Multimodal matching relations between image and sentence. The words and phrases, such as "grass", "a red ball", and "small black and brown dog play with a red ball", correspond to the image areas of their grounding meanings. The sentence "small black and brown dog play with red ball in the grass" expresses the meaning of the whole image.
whole sentence "small black and brown dog play with a red ball in the grass", expressing a com- plete meaning, associates with the whole image. These matching relations should be all taken into consideration for an accurate multimodal matching between image and sen- tence. Recently, much research work focuses on modeling the image and sentence matching relation at the specific lev-
1. Introduction
Associating image with natural language sentence plays
