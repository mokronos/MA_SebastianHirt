\chapter{Dataset}
\label{chap:dataset}
In this chapter, we present the dataset used in our work.
We use the \gls{scid} dataset \cite{ni_esim_2017}\footnote{The dataset can be downloaded here: https://eezkni.github.io/publications/ESIM.html.} as the base for our experiments.
The \gls{scid} dataset is suitable for our work, since the images contain text content on screenshots, different distortion levels and \gls{mos} values for each image.
Among the available screen content datasets mentioned in the literature \cite{iqa_survey_2020}, we find that other options are either inaccessible or fail to fulfill all the necessary criteria we require for our research.
An overview of the 40 reference images of the dataset can be seen in \autoref{fig:dataset_overview}.
Additionally, the dataset contains 1800 distorted images, which we discuss in the next section in detail.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{reference_images}
    \caption{Overview of the dataset.}
    \label{fig:dataset_overview}
\end{figure}


\section{Distortion types}
\label{sec:dataset_distortion_types}


The 1800 distorted images are generated from the 40 reference images.
They are distorted with 9 different distortion types, each with 5 different distortion quality levels.
In \autoref{tab:distortion_types}, we list the distortion types with a short description.


\begin{table}[h!]
\centering
\caption{Distortion types used in the dataset.}
\begin{tabular}{|p{6cm}|l|p{6cm}|}
\hline
\textbf{Distortion Type} & \textbf{Abbreviation} & \textbf{Description} \\
\hline
Gaussian Noise & GN & Addition of noise to an image using a Gaussian distribution \\
\hline
Gaussian Blur & GB & Blurring of an image using a Gaussian function \\
\hline
Motion Blur & MB & Blurring of an image due to movement \\
\hline
Contrast Change & CC & Changes in the contrast of an image \\
\hline
Joint Photographic Experts Group & JPEG & Image compression standard \\
\hline
Joint Photographic Experts Group 2000 & JPEG2000 & Image compression standard \\
\hline
Color Saturation Change & CSC & Changes in the color saturation of an image \\
\hline
High Efficiency Video Coding-Screen Content Coding & HEVC-SCC & Video compression standard for screen content \\
\hline
Color Quantization with Dithering & CQD & Reduction of colors in an image using dithering \\
\hline
\end{tabular}
\label{tab:distortion_types}
\end{table}


The images distorted by \gls{gn} have noise added with zero mean and standard deviations of $0.001$, $0.005$, $0.01$, $0.05$ and $0.1$ for each quality level respectively.
The images distorted by \gls{gb} are blurred with a Gaussian kernel.
The size of the kernel is $5\times5$ with standard deviations of $0.58$, $0.76$, $0.96$, $1.2$ and $2.1$ for each quality level respectively.
Images distorted by \gls{mb} are blurred with a motion kernel, which simulates motion blur.
The 'theta' parameter, which controls the degree of angle in a counter-clockwise direction, is set to zero and the 'len' parameter, which determines the length of the movement of the simulated camera, is set to $2$, $3.4$, $4$, $5.5$ and $6.4$ respectively.
The \gls{cc} distortion scales certain pixel values in the reference image to new values to change the contrast.
The scaling is applied for the ranges $[0,1] \rightarrow [0.3,0.5]$, $[0,1] \rightarrow [0.1,0.7]$, $[0.1,0.8] \rightarrow [0.1,0.9]$, $[0.2,0.8] \rightarrow [0.1,0.8]$ and $[0.2,0.7] \rightarrow [0,1]$ respectively.
So for the first quality level, all pixel values (from 0 to 1) are scaled to values between 0.3 and 0.5 of the maximum pixel intensity.
For the \gls{jpeg} compression, the images are compressed by the image compression algorithm with quality factors $75$, $35$, $18$, $8$ and $5$ respectively.
The \gls{jpeg2000} compression is applied with compression ratios of $0.08$, $0.045$, $0.02$, $0.015$ and $0.01$ respectively.
The \gls{csc} distortion keeps the luminance component of the images constant, but scales the chrominance components by the factors $0.96$, $0.72$, $0.58$, $0.42$ and $0.1$ respectively.
The \gls{hevcscc} distortion is applied by using the \gls{hevc} codec with the screen content configuration on the images with \glspl{qp} set to $16$, $36$, $40$, $42$ and $48$ respectively.
The \gls{cqd} distortion is applied by reducing the number of colors available in the image to $30$, $28$, $25$, $10$ and $5$ respectively.
More detailed descriptions of the implementation of the distortions can be found in the supporting file included with the dataset.

The different distortion types, at their most severe quality level, can be seen in \autoref{fig:distortion_types}, applied to the image in \autoref{fig:img29}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{../../data/raw/scid/ReferenceSCIs/SCI29.png}
    \caption{Reference image SCI29}
    \label{fig:img29}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{../../data/raw/scid/DistortedSCIs/SCI29_1_5.png}
        \caption{Gaussian Noise}
        \label{fig:distortion_type_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{../../data/raw/scid/DistortedSCIs/SCI29_2_5.png}
        \caption{Gaussian Blur}
        \label{fig:distortion_type_2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{../../data/raw/scid/DistortedSCIs/SCI29_3_5.png}
        \caption{Motion Blur}
        \label{fig:distortion_type_3}
    \end{subfigure}
    \newline
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{../../data/raw/scid/DistortedSCIs/SCI29_4_5.png}
        \caption{Contrast Change}
        \label{fig:distortion_type_4}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{../../data/raw/scid/DistortedSCIs/SCI29_5_5.png}
        \caption{JPEG Compression}
        \label{fig:distortion_type_5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{../../data/raw/scid/DistortedSCIs/SCI29_6_5.png}
        \caption{JPEG2000 Compression}
        \label{fig:distortion_type_6}
    \end{subfigure}
    \newline
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{../../data/raw/scid/DistortedSCIs/SCI29_7_5.png}
        \caption{Color Saturation Change}
        \label{fig:distortion_type_7}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{../../data/raw/scid/DistortedSCIs/SCI29_8_5.png}
        \caption{HEVC Screen Content Coding}
        \label{fig:distortion_type_8}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{../../data/raw/scid/DistortedSCIs/SCI29_9_5.png}
        \caption{Color Quantization with dithering}
        \label{fig:distortion_type_9}
    \end{subfigure}
    \caption{Distorted image 29 with 9 different distortion types and distortion level 5.}
    \label{fig:distortion_types}
\end{figure}

The impact of different distortion types on text within an image is evident.
Among the various distortions, alterations in contrast or color have minimal effect on text legibility for human readers.
Conversely, distortions such as \gls{gn}, \gls{gb} and \gls{mb} can render the text completely unreadable to the human eye.
We might expect that the distortions that affect the text the most, will also affect the \gls{ocr} the most.
It is to note, that all distortions are monotonically decreasing with the quality level from 1 to 5.
The only exception is \gls{cc}.
As illustrated in \autoref{fig:cc_levels}, \gls{cc} does not display a clear pattern of variation from low contrast to high contrast, or any similar trend.
This is important for when we analyze the trends of the \gls{mos} over the different quality levels.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.18\textwidth}
        \includegraphics[width=\textwidth]{../../data/raw/scid/DistortedSCIs/SCI01_4_1.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.18\textwidth}
        \includegraphics[width=\textwidth]{../../data/raw/scid/DistortedSCIs/SCI01_4_2.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.18\textwidth}
        \includegraphics[width=\textwidth]{../../data/raw/scid/DistortedSCIs/SCI01_4_3.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.18\textwidth}
        \includegraphics[width=\textwidth]{../../data/raw/scid/DistortedSCIs/SCI01_4_4.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.18\textwidth}
        \includegraphics[width=\textwidth]{../../data/raw/scid/DistortedSCIs/SCI01_4_5.png}
    \end{subfigure}
    \caption{Distorted image 1 with different levels of \gls{cc}, quality levels from left to right: 1, 2, 3, 4, 5.}
    \label{fig:cc_levels}
\end{figure}
        

\section{Labeling}
\label{sec:dataset_labeling}

The dataset doesn't have any text labels.
Thus we label the dataset ourselves.
First, we type the text on each image into a text file, starting on the top left and ending on the bottom right.
Our order of labeling was always dependent on the top left corner of a text element.
Thus if two paragraphs were side by side we always label the first line of the left paragraph first, then the first line of the right paragraph, then the second line of the left paragraph and so on.
This way we could ensure that we have a unifying representation of the text with regards to the two \gls{ocr} algorithms we used.
The algorithms have different modes for combining text elements, and thus produce differently connect paragraphs, if the paragraph mode is used.
To get a generalized result we use the option of raw text detection and text recognition to get the bounding boxes and the text for each element.
We then use the bounding boxes to get the text in the correct order.

Both \gls{ocr} algorithms have a paragraph mode, which combines text elements into paragraphs and then orders these paragraphs from top to bottom.
This seems reasonable, but we found that the algorithms sometimes consider different elements as part of the same paragraph.
Thus the ordering in the \gls{gt} becomes difficult to not bias towards one of the algorithms.
Another option is to predict the text and the corresponding bounding boxes and then order them for both algorithms.

The \texttt{readtext} method of EasyOCR returns the text elements and their bounding boxes.
They are ordered pretty well from top left to bottom right, but sometimes multiple words are merged together in one prediction.
This is fine, because the final text is just a concatenation of all the text elements with spaces in between, so it does not make a difference.
We tried to adjust the \texttt{width\_ths} property of the \texttt{readtext}, which controls when two text elements are merged together.
However, we were not able to find a value that separates the words well for most images.
With a value that is too low, single letters were not merged into words, and left bounding boxes for each letter.
So in general we found it better to use the default parameters of the \texttt{readtext} method.

With \texttt{pytesseract's} \texttt{image\_to\_data} method we can get the bounding boxes and the text elements with Tesseract \gls{ocr}.
However the bounding boxes are ordered more by paragraphs than lines.
Thus we decided to reorder the bounding boxes to better fit what EasyOCR returns and what our \gls{gt} order looks like.
Tesseract \gls{ocr} offers multiple engine modes.
Either legacy engine only, neural nets \gls{lstm} engine only or Legacy + \gls{lstm} engine.
We decided to use the pure \gls{lstm} engine mode, as it was the default and the most recent engine.
We used the Tesseract \gls{ocr} version 4.1.1.


EasyOCR determines if two words are in the same line by the parameter \texttt{ycenter\_ths}.
It denotes the maximum shift in the y direction between two words to be considered in the same line.
While it uses the center of the bounding box, for Tesseract \gls{ocr} we used the top 50\% of the highest bounding box in the line as the tolerance for other words belonging to the line.
The results are pretty similar.

--- In ground truth margins of what is in the same line are way slimmer.

--- The important thing is however that both algorithms have similar rules, so that they can be compared fairly.

--- images for each default order, and modified order, and order of gt maybe


In \autoref{fig:ref29} we can see that it is sometimes difficult to say which text element is higher than another, especially if the text elements have different font sizes.
Thus we decided against using one of the \gls{ocr} algorithms to predict the text and then correct it to create the \gls{gt}, to not introduce bias towards one of the algorithms with regard to the positions of the text elements.
We used the images, a ruler and our eyes to determine the correct order of the text elements.

--- Select based on how hard to label

--- Select based on how much text/objects

--- very subjective

--- different metric with IoU and CER combined might be helpful

--- go through all the distortions and make hypothesis on how they affect the ocr, check for literature on this

--- check later in evaluation if true/false

--- mos vs dmos / single stimulus vs double stimulus

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../../data/raw/scid/ReferenceSCIs/SCI29.png}
    \caption{Reference image 29.}
    \label{fig:ref29}
\end{figure}

\section{Analysis}
\label{sec:dataset_analysis}

In \autoref{fig:cer_vs_mos} the cer is plotted against the MOS.
It shows the \gls{cer} and \gls{mos} of all 1800 distorted images compared to their reference image.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{mosvster_all}
    \caption{\gls{cer} of all distorted images compared to their reference image plotted against the \gls{mos}. Probably not in final version.}
    \label{fig:cer_vs_mos}
\end{figure}

In \autoref{fig:cer_vs_mos_456} the \gls{cer} is plotted against the \gls{mos}.
We are only using a selection of images in this plot.
The images with the ID's $[1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 15, 18, 20, 21, 24, 29]$ have their main focus on text, and have a relatively simple text structure.
Thus those images are used in experiments that involve the \gls{mos}.

--- even if images have only one line of text, cer might still be a good metric, as distortion affects that line the same as it affects the whole image.

--- so if mos is low because objects look bad, if cer is a good metric, it should be low as well, as the text is also affected by the distortion

--- but distortions generally dont affect text the same as objects

--- our objective is more to check if cer is a good metric to simulate mos for readability of text, although it might be less consistent


\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{mosvster__456_22_29__1_4}
    \caption{\gls{cer} of a selection of distorted images compared to their reference image. Probably not in final version}
    \label{fig:cer_vs_mos_456}
\end{figure}

\section{Codec Encoding}
\label{sec:dataset_codec}

In this section we first describe the extension of the dataset through encoding with the \gls{hevc} and \gls{vvc} codecs.
Additionally we detail the method we then use to compare the codecs by using the $\text{CER}_{\text{c}}$ and the \gls{bdrate}.

--- authors name it mos, but it should be dmos, because double stimulus \cite{mos_dmos_1993}

--- still use mos for rest of work

\subsection{High Efficiency Video Coding}
\label{subsec:hevc}

The \gls{hevc} \cite{hevc_2012} is one of the newest video codecs.
It is the successor of the H.264/MPEG-4 AVC codec.
The main improvements were the leveraging of parallel processing architecture in modern devices and addressing higher resolutions.
The codec uses the conventional approach of dividing the image into blocks for encoding and leveraging sequential frames to only encode the difference instead of the whole frame.
However it also employs a number of new techniques to provide around 50\% bit rate savings for equivalent quality.

\subsection{Versatile Video Coding}
\label{subsec:vvc}

The \gls{vvc} \cite{vvc_2021} is the successor of the \gls{hevc} and the most recent video codec.
It has another 50\% bit rate savings compared to the \gls{hevc} for equivalent quality.
Additionally, the versatility of the codec enables it to be used for a wider range of applications, including $360^{\circ}$ immersive video, high dynamic range, adaptive streaming with resolution changes and many more.


It is to note, that we use these codecs on images instead of videos, which means that we are not leveraging the full potential of the videos codecs.
Furthermore, we encoded the images with the default and the \gls{scc} configuration of the codecs to observe the difference.
The most important difference to the other distorted images is that there are no subjective scores available for these images.
However, we can use more images for the comparison of the codecs, as we do not need to look out for objects overshadowing the text in the subjective scores.
The images used for the experiments with the codecs have the ID's $[1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 27, 29]$.
For images encoded with these codecs the $\text{CER}_{\text{c}}$ can be calculated after predicting the text with the \gls{ocr} algorithms.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../../exp/bjontegaard_example.pdf}
    \caption{Example of the \gls{bdrate} calculation with dummy values. Adapted from \cite{bdrate_beyond_2022}}
    \label{fig:bdrate_example}
\end{figure}

Those $\text{CER}_{\text{c}}$ values can be plotted against the bitrate of the encoded images, see \autoref{fig:bdrate_example} for an example.
The different qualities can be denoted by $\text{i}$ and the different codecs by $\text{k}$.
In our case the qualities describe the \glspl{qp} values $[35, 40, 45, 50]$ and the codecs are $[\text{HM}, \text{VTM}]$.

--- deviating from VVC common test conditions, due to lack of changes in the CER for lower QP values

Thus the points can be denoted as $\left(R_{\text{k,i}}, M_{\text{k,i}}\right)$ with $R_{\text{k,i}}$ being the bitrate and $D_{\text{k,i}}$ being the metric, in our case the $\text{CER}_{\text{c}}$.
Later, those points are averaged over all images to get the average bitrate and the average $\text{CER}_{\text{c}}$ for each codec.
The following section describes the calculation of the \gls{bdrate}, which can quantify the difference between the two curves of the codecs.


To summarize, the original dataset has a \gls{mos} for each image with various distortions.
We are expanding the dataset by encoding the images with the \gls{hevc} and \gls{vvc} codecs.
The $\Delta R$ can then be calculated to compare two codecs and evaluate if the \gls{ocr} algorithms can be used as a approximate \gls{gt} for the \gls{bdrate} calculation.
Now that we have all the parts to describe the experiments, we can evaluate and discuss the results in the next chapter.
