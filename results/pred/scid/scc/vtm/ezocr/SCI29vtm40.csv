left,top,right,bottom,text,conf
266,12,378,40,Abstract,1.0
865,21,1079,39,small black and brown dog play with,0.84
1087,21,1205,39,red ball in the grass,0.99
1017,73,1115,85,play with ared ballin,0.73
64,73,608,102,In this paper; we propose multimodal convolutional neu-,0.78
1054,88,1076,96,2r55,0.44
34,104,308,130,ral networks (m-CNNs) for,0.67
306,98,607,134,matching image and sentence:,0.77
995,117,1065,129,small black anc,0.59
1067,117,1117,129,brown dog,0.71
1013,131,1055,143,play with,0.76
36,129,600,158,Our m-CNN provides an end-to-end framework with convo,0.76
34,159,608,188,"lutional architectures to exploit image representation, word",0.89
1123,179,1169,191,black and,0.71
983,185,1023,199,red ball,0.96
33,186,609,218,"composition, and the matching relations between the two",0.78
1122,194,1170,202,brown doz,0.41
34,216,142,242,modalities:,0.77
148,216,608,247,More specifically; it consists of one image CNN,0.69
33,242,606,275,encoding the image content and one matching CNN mod-,0.81
936,262,960,270,Erass,0.37
90,275,556,304,the joint representation of image and sentence.,0.88
566,276,608,300,The,1.0
33,300,608,333,matching CNN composes different semantic fragments from,0.79
656,310,718,338,Figure,0.66
751,304,1229,340,Multimodal   matching  relations   between  image  and,0.55
36,331,609,361,words and learns the inter-modal relations between image,0.98
657,343,741,359,sentence.,0.97
752,336,1048,364,"The   words and phrases, such as",0.71
1060,340,1130,364,grass,1.0
1185,339,1227,359,red,1.0
34,359,608,391,"and the composed fragments at different levels, thus ful-",0.72
654,361,726,389,"ball"",",0.99
763,365,797,385,and,1.0
835,362,982,389,'small black,0.7
989,363,1035,388,and,1.0
1041,367,1111,385,brown,1.0
1120,366,1164,390,dog,1.0
1170,366,1228,390,play,1.0
33,384,561,420,ly exploit the matching relations between image and,0.66
565,397,605,413,sen-,0.74
657,391,711,411,with,0.98
747,391,789,411,red,1.0
796,388,1230,417,"ball"", correspond to the image areas of their",0.87
34,417,609,448,tence. Experimental results demonstrate that the proposed,0.71
656,412,849,443,grounding   meanings,0.8
883,417,921,437,The,1.0
931,419,1009,437,sentence,0.77
1030,414,1176,441,small black,0.98
1184,416,1228,440,and,1.0
39,455,67,471,m-,0.98
66,446,608,474,CNNs can effectively capture the information necessary,0.74
657,445,725,465,brown,1.0
734,444,778,470,dog,1.0
784,444,842,468,play,1.0
851,445,905,465,with,1.0
940,442,984,466,red,1.0
991,445,1045,465,ball,1.0
1057,445,1087,465,in,1.0
1094,443,1216,470,the grass,0.99
32,472,375,505,for image and sentence matching:,0.74
388,474,564,502,More specifically;,0.9
571,483,609,499,our,0.97
654,468,968,496,expresses the meaning of the whole,0.8
32,503,130,534,proposed,1.0
134,501,606,534,m-CNNs significantly outperform the state-of-,0.68
34,532,606,563,the-art approaches for bidirectional image and sentence re -,0.74
656,522,720,546,whole,1.0
729,525,815,545,sentence,1.0
835,522,982,549,'small black,0.56
989,523,1035,548,and,1.0
1040,523,1228,551,brown dog play,0.95
34,560,492,587,trieval on the Flickr8K and Flickr3OK datasets:,0.55
657,555,711,575,with,1.0
747,555,789,575,red,1.0
797,555,853,575,ball,1.0
863,555,893,575,in,1.0
900,553,1022,578,the grass,0.98
1041,549,1152,580,expressing,0.95
1157,555,1227,575,a com-,1.0
656,576,914,609,"plete meaning, associates",0.92
920,580,1006,604,with the,0.99
1012,579,1151,610,whole image.,0.85
1166,580,1228,604,These,1.0
655,604,1230,636,matching relations should be all taken into consideration for,0.84
36,646,226,676,1. Introduction,0.99
656,636,890,662,an accurate multimodal,0.77
890,632,1229,668,matching between image and sen-,1.0
654,662,1229,694,"tence. Recently, much research work focuses on modeling",0.8
66,695,608,720,Associating image with natural language sentence plays,0.76
656,690,1228,720,the image and sentence matching relation at the specific lev-,0.81
997,71,1020,88,dog !,0.23
36,270,91,306,eling,1.0
967,466,1030,499,image.,0.79
